{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGTEHBiBujqZ",
        "outputId": "a64d1b18-23af-4f66-e9a8-53dc2324f860"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.51.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sIfChWnEueiW",
        "outputId": "8a9e3374-8093-45a4-a3a5-96fa0605cb30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-cifar10-25epochs</strong> at: <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/hupnoxv2' target=\"_blank\">https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/hupnoxv2</a><br> View project at: <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2' target=\"_blank\">https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260201_150443-hupnoxv2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260201_150603-0twct87l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/0twct87l' target=\"_blank\">resnet18-cifar10-25epochs</a></strong> to <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2' target=\"_blank\">https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/0twct87l' target=\"_blank\">https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/0twct87l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading CIFAR-10 dataset...\n",
            "Training samples: 50000, Test samples: 10000\n",
            "Initializing ResNet18 model...\n",
            "Counting FLOPs...\n",
            "Total FLOPs: 557,208,586 (557.21 MFLOPs)\n",
            "\n",
            "Starting training for 25 epochs...\n",
            "Epoch [1/25] | Train Loss: 1.3352 | Train Acc: 51.28% | Test Loss: 1.2315 | Test Acc: 60.58% | Time: 46.94s\n",
            "New best accuracy: 60.58%\n",
            "Epoch [2/25] | Train Loss: 0.8470 | Train Acc: 70.20% | Test Loss: 0.8698 | Test Acc: 70.44% | Time: 44.86s\n",
            "New best accuracy: 70.44%\n",
            "Epoch [3/25] | Train Loss: 0.6632 | Train Acc: 76.97% | Test Loss: 0.7372 | Test Acc: 75.05% | Time: 45.75s\n",
            "New best accuracy: 75.05%\n",
            "Epoch [4/25] | Train Loss: 0.5512 | Train Acc: 80.85% | Test Loss: 0.5848 | Test Acc: 80.42% | Time: 44.40s\n",
            "New best accuracy: 80.42%\n",
            "Epoch [5/25] | Train Loss: 0.4826 | Train Acc: 83.34% | Test Loss: 0.5903 | Test Acc: 80.34% | Time: 44.59s\n",
            "Epoch [6/25] | Train Loss: 0.4234 | Train Acc: 85.33% | Test Loss: 0.5243 | Test Acc: 82.71% | Time: 45.74s\n",
            "New best accuracy: 82.71%\n",
            "Epoch [7/25] | Train Loss: 0.3851 | Train Acc: 86.81% | Test Loss: 0.4567 | Test Acc: 84.73% | Time: 44.47s\n",
            "New best accuracy: 84.73%\n",
            "Epoch [8/25] | Train Loss: 0.3446 | Train Acc: 88.01% | Test Loss: 0.4306 | Test Acc: 86.08% | Time: 45.29s\n",
            "New best accuracy: 86.08%\n",
            "Epoch [9/25] | Train Loss: 0.3098 | Train Acc: 89.33% | Test Loss: 0.4152 | Test Acc: 86.19% | Time: 45.04s\n",
            "New best accuracy: 86.19%\n",
            "Epoch [10/25] | Train Loss: 0.2778 | Train Acc: 90.56% | Test Loss: 0.4271 | Test Acc: 85.97% | Time: 44.73s\n",
            "Epoch [11/25] | Train Loss: 0.2478 | Train Acc: 91.48% | Test Loss: 0.4081 | Test Acc: 86.82% | Time: 45.03s\n",
            "New best accuracy: 86.82%\n",
            "Epoch [12/25] | Train Loss: 0.2184 | Train Acc: 92.45% | Test Loss: 0.3737 | Test Acc: 88.02% | Time: 44.94s\n",
            "New best accuracy: 88.02%\n",
            "Epoch [13/25] | Train Loss: 0.1970 | Train Acc: 93.08% | Test Loss: 0.3441 | Test Acc: 89.11% | Time: 44.60s\n",
            "New best accuracy: 89.11%\n",
            "Epoch [14/25] | Train Loss: 0.1720 | Train Acc: 93.96% | Test Loss: 0.3490 | Test Acc: 89.35% | Time: 44.74s\n",
            "New best accuracy: 89.35%\n",
            "Epoch [15/25] | Train Loss: 0.1521 | Train Acc: 94.73% | Test Loss: 0.3242 | Test Acc: 90.39% | Time: 45.05s\n",
            "New best accuracy: 90.39%\n",
            "Epoch [16/25] | Train Loss: 0.1285 | Train Acc: 95.47% | Test Loss: 0.3284 | Test Acc: 90.20% | Time: 44.92s\n",
            "Epoch [17/25] | Train Loss: 0.1067 | Train Acc: 96.30% | Test Loss: 0.3286 | Test Acc: 90.69% | Time: 45.31s\n",
            "New best accuracy: 90.69%\n",
            "Epoch [18/25] | Train Loss: 0.0903 | Train Acc: 96.81% | Test Loss: 0.3138 | Test Acc: 91.38% | Time: 44.81s\n",
            "New best accuracy: 91.38%\n",
            "Epoch [19/25] | Train Loss: 0.0741 | Train Acc: 97.43% | Test Loss: 0.3301 | Test Acc: 91.40% | Time: 44.73s\n",
            "New best accuracy: 91.40%\n",
            "Epoch [20/25] | Train Loss: 0.0627 | Train Acc: 97.81% | Test Loss: 0.3277 | Test Acc: 91.56% | Time: 45.87s\n",
            "New best accuracy: 91.56%\n",
            "Epoch [21/25] | Train Loss: 0.0525 | Train Acc: 98.20% | Test Loss: 0.3205 | Test Acc: 91.72% | Time: 44.67s\n",
            "New best accuracy: 91.72%\n",
            "Epoch [22/25] | Train Loss: 0.0459 | Train Acc: 98.49% | Test Loss: 0.3283 | Test Acc: 91.88% | Time: 44.55s\n",
            "New best accuracy: 91.88%\n",
            "Epoch [23/25] | Train Loss: 0.0400 | Train Acc: 98.61% | Test Loss: 0.3261 | Test Acc: 91.97% | Time: 45.51s\n",
            "New best accuracy: 91.97%\n",
            "Epoch [24/25] | Train Loss: 0.0353 | Train Acc: 98.85% | Test Loss: 0.3285 | Test Acc: 91.92% | Time: 44.56s\n",
            "Epoch [25/25] | Train Loss: 0.0350 | Train Acc: 98.86% | Test Loss: 0.3261 | Test Acc: 91.90% | Time: 44.28s\n",
            "\n",
            "Training completed! Best Test Accuracy: 91.97%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_accuracy</td><td>▁▁▄▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>batch_loss</td><td>█▆▅▆▅▅▄▅▄▃▃▄▃▃▃▃▃▃▂▂▂▃▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>best_test_accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>epoch_time</td><td>█▃▅▁▂▅▁▄▃▂▃▃▂▂▃▃▄▂▂▅▂▂▄▂▁</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▃▄▅▅▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>+62</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_accuracy</td><td>98.88393</td></tr><tr><td>batch_loss</td><td>0.01224</td></tr><tr><td>best_test_accuracy</td><td>91.97</td></tr><tr><td>epoch</td><td>24</td></tr><tr><td>epoch_time</td><td>44.27794</td></tr><tr><td>learning_rate</td><td>0</td></tr><tr><td>test_accuracy</td><td>91.9</td></tr><tr><td>test_loss</td><td>0.32611</td></tr><tr><td>train_accuracy</td><td>98.856</td></tr><tr><td>train_loss</td><td>0.03501</td></tr><tr><td>+62</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-cifar10-25epochs</strong> at: <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/0twct87l' target=\"_blank\">https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2/runs/0twct87l</a><br> View project at: <a href='https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2' target=\"_blank\">https://wandb.ai/pancholisaumya-iit/cifar10-cnn-lab2</a><br>Synced 5 W&B file(s), 5 media file(s), 10 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260201_150603-0twct87l/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All visualizations have been logged to Weights & Biases!\n",
            "Check your wandb dashboard for gradient flow and weight update visualizations.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import time\n",
        "\n",
        "# Initialize Wandb\n",
        "wandb.init(\n",
        "    project=\"cifar10-cnn-lab2\",\n",
        "    name=\"resnet18-cifar10-25epochs\",\n",
        "    config={\n",
        "        \"architecture\": \"ResNet18\",\n",
        "        \"dataset\": \"CIFAR-10\",\n",
        "        \"epochs\": 25,\n",
        "        \"batch_size\": 128,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        ")\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "\n",
        "# Custom CIFAR-10 Dataset Class\n",
        "class CustomCIFAR10Dataset(Dataset):\n",
        "    \"\"\"Custom Dataset wrapper for CIFAR-10\"\"\"\n",
        "\n",
        "    def __init__(self, root='./data', train=True, transform=None, download=True):\n",
        "        self.cifar_data = torchvision.datasets.CIFAR10(\n",
        "            root=root,\n",
        "            train=train,\n",
        "            download=download,\n",
        "            transform=None\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.cifar_data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Define CNN Model (ResNet18)\n",
        "class ResNet18CIFAR(nn.Module):\n",
        "    \"\"\"ResNet18 adapted for CIFAR-10\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18CIFAR, self).__init__()\n",
        "        # Load pretrained ResNet18 and modify for CIFAR-10\n",
        "        self.model = torchvision.models.resnet18(weights=None)\n",
        "        # Modify first conv layer for 32x32 images\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.model.maxpool = nn.Identity()  # Remove maxpool for small images\n",
        "        # Modify final layer for 10 classes\n",
        "        self.model.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# FLOPs Counting Function\n",
        "def count_flops(model, input_size=(1, 3, 32, 32), device='cpu'):\n",
        "    \"\"\"\n",
        "    Count FLOPs for the model\n",
        "    \"\"\"\n",
        "    def conv_flops_counter_hook(conv_module, input, output):\n",
        "        batch_size = output.shape[0]\n",
        "        output_dims = list(output.shape[2:])\n",
        "\n",
        "        kernel_dims = list(conv_module.kernel_size)\n",
        "        in_channels = conv_module.in_channels\n",
        "        out_channels = conv_module.out_channels\n",
        "        groups = conv_module.groups\n",
        "\n",
        "        filters_per_channel = out_channels // groups\n",
        "        conv_per_position_flops = int(np.prod(kernel_dims)) * in_channels * filters_per_channel\n",
        "\n",
        "        active_elements_count = batch_size * int(np.prod(output_dims))\n",
        "        overall_conv_flops = conv_per_position_flops * active_elements_count\n",
        "\n",
        "        bias_flops = 0\n",
        "        if conv_module.bias is not None:\n",
        "            bias_flops = out_channels * active_elements_count\n",
        "\n",
        "        overall_flops = overall_conv_flops + bias_flops\n",
        "        conv_module.__flops__ += int(overall_flops)\n",
        "\n",
        "    def linear_flops_counter_hook(linear_module, input, output):\n",
        "        batch_size = input[0].shape[0]\n",
        "        num_flops = batch_size * linear_module.in_features * linear_module.out_features\n",
        "\n",
        "        if linear_module.bias is not None:\n",
        "            num_flops += batch_size * linear_module.out_features\n",
        "\n",
        "        linear_module.__flops__ += int(num_flops)\n",
        "\n",
        "    def bn_flops_counter_hook(bn_module, input, output):\n",
        "        batch_size = input[0].shape[0]\n",
        "        num_elements = input[0].numel() // batch_size\n",
        "        bn_module.__flops__ += int(2 * num_elements * batch_size)\n",
        "\n",
        "    def relu_flops_counter_hook(relu_module, input, output):\n",
        "        batch_size = input[0].shape[0]\n",
        "        num_elements = input[0].numel() // batch_size\n",
        "        relu_module.__flops__ += int(num_elements * batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    hooks = []\n",
        "\n",
        "    def add_hooks(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.__flops__ = 0\n",
        "            hooks.append(m.register_forward_hook(conv_flops_counter_hook))\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.__flops__ = 0\n",
        "            hooks.append(m.register_forward_hook(linear_flops_counter_hook))\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "            m.__flops__ = 0\n",
        "            hooks.append(m.register_forward_hook(bn_flops_counter_hook))\n",
        "        elif isinstance(m, (nn.ReLU, nn.ReLU6)):\n",
        "            m.__flops__ = 0\n",
        "            hooks.append(m.register_forward_hook(relu_flops_counter_hook))\n",
        "\n",
        "    model.apply(add_hooks)\n",
        "\n",
        "    input_tensor = torch.randn(input_size).to(device) # Move input tensor to the correct device\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "    total_flops = 0\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, '__flops__'):\n",
        "            total_flops += m.__flops__\n",
        "\n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    return total_flops\n",
        "\n",
        "\n",
        "# Gradient Flow Tracking\n",
        "def plot_grad_flow(named_parameters, epoch):\n",
        "    \"\"\"\n",
        "    Track gradient flow through the network\n",
        "    \"\"\"\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "\n",
        "    for n, p in named_parameters:\n",
        "        if p.requires_grad and p.grad is not None and \"bias\" not in n:\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean().cpu().item())\n",
        "            max_grads.append(p.grad.abs().max().cpu().item())\n",
        "\n",
        "    # Log to wandb\n",
        "    grad_data = [[layer, avg, max_val] for layer, avg, max_val in zip(layers, ave_grads, max_grads)]\n",
        "    table = wandb.Table(data=grad_data, columns=[\"Layer\", \"Average Gradient\", \"Max Gradient\"])\n",
        "    wandb.log(\n",
        "        {\n",
        "            f\"gradient_flow_epoch_{epoch}\": wandb.plot.bar(table, \"Layer\", \"Average Gradient\",\n",
        "                                                        title=f\"Gradient Flow - Epoch {epoch}\"),\n",
        "            f\"max_gradient_flow_epoch_{epoch}\": wandb.plot.bar(table, \"Layer\", \"Max Gradient\",\n",
        "                                                             title=f\"Max Gradient Flow - Epoch {epoch}\")\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return ave_grads, max_grads\n",
        "\n",
        "\n",
        "# Weight Update Flow Tracking\n",
        "def track_weight_updates(model, old_weights, epoch):\n",
        "    \"\"\"\n",
        "    Track weight updates across epochs\n",
        "    \"\"\"\n",
        "    weight_changes = {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in old_weights and param.requires_grad:\n",
        "            change = (param.data - old_weights[name]).abs().mean().item()\n",
        "            weight_changes[name] = change\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb.log({f\"weight_update/{name}\": change for name, change in weight_changes.items()})\n",
        "\n",
        "    return weight_changes\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_epoch(model, trainloader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        if i % 50 == 49:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"batch_loss\": loss.item(),\n",
        "                    \"batch_accuracy\": 100. * correct / total\n",
        "                }\n",
        "            )\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "# Validation Function\n",
        "def validate(model, testloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = test_loss / len(testloader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "# Main Training Loop\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # Create custom datasets\n",
        "    print(\"Loading CIFAR-10 dataset...\")\n",
        "    trainset = CustomCIFAR10Dataset(root='./data', train=True, transform=transform_train, download=True)\n",
        "    testset = CustomCIFAR10Dataset(root='./data', train=False, transform=transform_test, download=True)\n",
        "\n",
        "    # Create dataloaders\n",
        "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\"Training samples: {len(trainset)}, Test samples: {len(testset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing ResNet18 model...\")\n",
        "    model = ResNet18CIFAR(num_classes=10).to(device)\n",
        "\n",
        "    # Count FLOPs\n",
        "    print(\"Counting FLOPs...\")\n",
        "    flops = count_flops(model, input_size=(1, 3, 32, 32), device=device)\n",
        "    print(f\"Total FLOPs: {flops:,} ({flops/1e6:.2f} MFLOPs)\")\n",
        "    wandb.config.update({\"total_flops\": flops, \"flops_millions\": flops/1e6})\n",
        "\n",
        "    # Watch model with wandb\n",
        "    wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Save old weights for tracking updates\n",
        "        old_weights = {name: param.data.clone() for name, param in model.named_parameters()}\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device, epoch)\n",
        "\n",
        "        # Validate\n",
        "        test_loss, test_acc = validate(model, testloader, criterion, device)\n",
        "\n",
        "        # Track gradient flow every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            plot_grad_flow(model.named_parameters(), epoch)\n",
        "\n",
        "        # Track weight updates\n",
        "        weight_changes = track_weight_updates(model, old_weights, epoch)\n",
        "\n",
        "        # Learning rate step\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # Log metrics\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_accuracy\": test_acc,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "                \"epoch_time\": epoch_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{config.epochs}] | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n",
        "              f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            print(f\"New best accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    print(f\"\\nTraining completed! Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    wandb.log({\"best_test_accuracy\": best_acc})\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    print(\"\\nAll visualizations have been logged to Weights & Biases!\")\n",
        "    print(\"Check your wandb dashboard for gradient flow and weight update visualizations.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}